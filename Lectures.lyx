#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{tikz}
\usepackage{mathpazo}
\end_preamble
\use_default_options true
\maintain_unincluded_children no
\language american
\language_package default
\inputencoding utf8
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 1cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Part*
Branching Processes
\end_layout

\begin_layout Section
17.3.2025
\end_layout

\begin_layout Standard
We start with a probability space:
 
\begin_inset Formula $\mathcal{F}=\left(\Omega,\sigma,\mathbb{P}\right)$
\end_inset

.
 
\begin_inset Formula $a\in\Omega$
\end_inset

 is a single event in our game,
 such as the result of a roll of a die.
 
\begin_inset Formula $\sigma$
\end_inset

 is a 
\begin_inset Formula $\sigma$
\end_inset

-algebra,
 over which we define our probability.
 If we can,
 we will use the power set of 
\begin_inset Formula $\Omega$
\end_inset

,
 also known as 
\begin_inset Formula $2^{\Omega}$
\end_inset

.
 Otherwise,
 we will use whatever algebra makes sense.
 
\begin_inset Formula $P$
\end_inset

 would be the probability measure.
\end_layout

\begin_layout Standard
Usually,
 we will have 
\begin_inset Formula $\Omega=\mathbb{N}$
\end_inset

,
 where we can use the power set,
 and when 
\begin_inset Formula $\Omega=\mathbb{R}$
\end_inset

,
 we will use the Borel measure.
\end_layout

\begin_layout Subsection
Random Variables
\end_layout

\begin_layout Standard
We are interested in functions 
\begin_inset Formula $X:\Omega\to\mathbb{R}$
\end_inset

,
 because we mostly count things,
 although in general we could replace 
\begin_inset Formula $\mathbb{R}$
\end_inset

 with other spaces that are measurable.
\end_layout

\begin_layout Standard
We call 
\begin_inset Formula $X$
\end_inset

 a 
\emph on
random variable
\emph default
 if for any 
\begin_inset Formula $I$
\end_inset

 open in 
\begin_inset Formula $\mathbb{R}$
\end_inset

,
 
\begin_inset Formula $X^{-1}\text{\left(I\right)\ensuremath{\in}\ensuremath{\sigma}}$
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $X,Y$
\end_inset

 are random variables,
 
\begin_inset Formula $X=Y$
\end_inset

 will denote functional equivalence,
 and 
\begin_inset Formula $X\overset{l}{=}Y$
\end_inset

 will denote equivalence 
\emph on
by law
\emph default
,
 which is defined as:
\begin_inset Formula 
\[
X\overset{l}{=}Y\Leftrightarrow\forall A\in\sigma\quad\mathbb{P}\left[X\left(A\right)\subset I\right]=\mathbb{P}\left[Y\left(A\right)\subset I\right]
\]

\end_inset


\end_layout

\begin_layout Subsection
Moments
\end_layout

\begin_layout Standard
The 
\emph on
probability distribution function of 
\begin_inset Formula $X$
\end_inset


\emph default
 (its PDF),
 denoted as 
\begin_inset Formula $p_{X}$
\end_inset

 can be given as:
\begin_inset Formula 
\[
p_{X}\left(s\right)=\mathbb{P}\left[X=s\right]
\]

\end_inset


\end_layout

\begin_layout Standard
which for discrete random variables will be 
\begin_inset Formula $p_{n}$
\end_inset

 as a series,
 and we'll hide 
\begin_inset Formula $X$
\end_inset

 to make it simpler to write.
\end_layout

\begin_layout Standard
For a continuous variable we would use the CDF 
\begin_inset Formula $\mathbb{P}\left[X\leq s\right]$
\end_inset

,
 and use its differential measure.
\end_layout

\begin_layout Standard
The average,
 or expectation of a random variable 
\begin_inset Formula $X$
\end_inset

 is given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bar{X}=\mathbb{E}\left[X\right]=\int p_{X}\left(s\right)sds
\]

\end_inset


\end_layout

\begin_layout Standard
We can have other random variables by setting functions over random variables,
 and then:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\overline{f\left(X\right)}=\mathbb{E}\left[f\left(X\right)\right]=\int f\left(s\right)p_{X}\left(s\right)ds
\]

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on

\begin_inset Formula $n$
\end_inset

-th moment
\emph default
 can be similarly written as:
\begin_inset Formula 
\[
M_{n}\left(X\right)=\mathbb{E}\left[X^{n}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on

\begin_inset Formula $n$
\end_inset

-th central moment
\emph default
 can be written as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C_{n}\left(X\right)=\mathbb{E}\left[\left(X-\bar{X}\right)^{n}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The first central moment is just 0,
 and the second is the 
\emph on
variance
\emph default
 
\begin_inset Formula $V\left[X\right]$
\end_inset

,
 which has to do with how wide the distribution is.
 The third,
 called 
\emph on
skewness
\emph default
,
 is the degree of how tilted the distribution is around its expected value.
\end_layout

\begin_layout Standard
The 
\emph on

\begin_inset Formula $n$
\end_inset

-th factorial moment
\emph default
 can be written as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
F_{n}\left(X\right)=\mathbb{E}\left[\prod_{i=0}^{n-1}\left(X-i\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
It will come in handy later in this course,
 which is when we'll discuss it more.
\end_layout

\begin_layout Subsection*
Theorem (which we don't prove here)
\end_layout

\begin_layout Standard
If two random variables 
\begin_inset Formula $X,Y$
\end_inset

 have the same moments 
\begin_inset Formula $\forall n\ M_{n}\left(X\right)=M_{n}\left(Y\right)$
\end_inset

,
 then 
\begin_inset Formula $X\overset{l}{=}Y$
\end_inset

.
\end_layout

\begin_layout Standard
If we only had a finite set of equal moments,
 we would have an infinite number of different distributions that fit,
 which is a classic interpolation problem,
 sometimes called the *hamburger problem*.
\end_layout

\begin_layout Subsection
Random Processes
\end_layout

\begin_layout Standard
Given an index set 
\begin_inset Formula $I$
\end_inset

 with elements 
\begin_inset Formula $i$
\end_inset

,
 
\begin_inset Formula $X_{i}$
\end_inset

 is a 
\emph on
random process
\emph default
 if for any 
\begin_inset Formula $i\in I$
\end_inset

,
 
\begin_inset Formula $X_{i}$
\end_inset

 is a random variable.
 The denotation seems confusing,
 but it's better to keep it simple,
 because we will have these all over the place.
\end_layout

\begin_layout Standard
\begin_inset Formula $I$
\end_inset

 will usually be the generation number if we have discrete time,
 so 
\begin_inset Formula $\mathbb{N}$
\end_inset

,
 or 
\begin_inset Formula $\mathbb{R}^{+}$
\end_inset

 if we talk about an index of time for continuous processes.
\end_layout

\begin_layout Paragraph
Distributions are not everything in processes.
\end_layout

\begin_layout Standard
Often,
 in random variables,
 we really only care about equality by law,
 because things that are similarly distributed are rather the same.
\end_layout

\begin_layout Standard
In random processes,
 we could have dependence,
 which would ruin this,
 as the game we play at stage 
\begin_inset Formula $n$
\end_inset

 could very likely depend on the result at earlier stages.
 In this case,
 knowing the total distribution at stage 
\begin_inset Formula $n$
\end_inset

 isn't enough,
 because we care about its dependence with other stages,
 so conditional probabilities matter.
\end_layout

\begin_layout Standard
The 
\emph on
autocorrelation of 
\begin_inset Formula $X$
\end_inset


\emph default
 is given by:
\begin_inset Formula 
\[
\varphi_{X}\left(s,t\right)=\mathbb{E}\left[X_{s}X_{t}\right]-\mathbb{E}\left[X_{s}\right]\mathbb{E}\left[X_{t}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Usually,
 we care about how strong the correlation is depending on 
\begin_inset Formula $s-t$
\end_inset

,
 such that correlations drop off as 
\begin_inset Formula $s-t$
\end_inset

 grows larger.
\end_layout

\begin_layout Standard
This has connections with Ergodicity,
 where taking ensemble averages is the same as taking samples from different far times.
\end_layout

\begin_layout Standard
A random process 
\begin_inset Formula $t_{n}$
\end_inset

 will be called a 
\emph on
time series
\emph default
 if 
\begin_inset Formula $n\in\mathbb{N}$
\end_inset

,
 
\begin_inset Formula $t_{n}\in\mathbb{R}^{+}$
\end_inset

and 
\begin_inset Formula $\forall n\ t_{n}<t_{n+1}$
\end_inset

.
 A time series would matter as the waiting time between events,
 mostly.
 We would care about the distribution of 
\emph on
dwell times 
\emph default
of 
\begin_inset Formula $t$
\end_inset

,
 
\begin_inset Formula $\theta_{n}=t_{n+1}-t_{n}$
\end_inset

 a lot.
 A second question would be how much time we must wait until the 
\begin_inset Formula $n$
\end_inset

-th event,
 which is simply 
\begin_inset Formula $t_{n}$
\end_inset

.
\end_layout

\begin_layout Standard
Given a time series we can get the dwell times,
 and given a series of positive random variables we can make them into a time series by adding them up.
\end_layout

\begin_layout Standard
A time series (defined by dwell times 
\begin_inset Formula $\theta_{n}$
\end_inset

,
 because of the above equivalence) will be called a 
\emph on
renewal process
\emph default
 if 
\begin_inset Formula $\left\{ \theta_{n}\right\} $
\end_inset

 are i.i.d.
\end_layout

\begin_layout Standard
The 
\emph on
counter of a time series 
\begin_inset Formula $t_{n}$
\end_inset


\emph default
 will be defined by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C_{s}=\max\left\{ n|t_{n}\leq s\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
It just tells us how many events happened by time 
\begin_inset Formula $t$
\end_inset

.
 This is the inverse indexing of knowing when the 
\begin_inset Formula $n$
\end_inset

-th event happened,
 which is 
\begin_inset Formula $t_{n}$
\end_inset

.
\end_layout

\begin_layout Subsection
Markovian and Stationary Processes
\end_layout

\begin_layout Standard
A Markovian process is a deep concept which takes its own course.
 These are processes where we can actually solve things.
\end_layout

\begin_layout Standard
A process 
\begin_inset Formula $X_{t}$
\end_inset

 will be called a 
\emph on
Markov process
\emph default
 if for any 
\begin_inset Formula $s>t>u$
\end_inset

,
 
\begin_inset Formula $P\left(X_{s}=x|X_{t}=y\right)=P\left(X_{s}=x|X_{t}=y,X_{u}=z\right)$
\end_inset

.
\end_layout

\begin_layout Standard
This means that any knowledge we need is the newest information.
 After we have new information,
 we simply don't care what happened before.
\end_layout

\begin_layout Standard
A non-Markovian process is for example,
 a dead time system with paralysis,
 since we need to know the last time we dropped.
\end_layout

\begin_layout Standard
A process 
\begin_inset Formula $X_{t}$
\end_inset

 will be called a 
\emph on
stationary process
\emph default
 if for any 
\begin_inset Formula $s>t$
\end_inset

 we have 
\begin_inset Formula $P\left(X_{s}=x|X_{t}=y\right)=F\left(x,y,s-t\right)$
\end_inset

.
 This means that a stationary process is obviously markovian.
\end_layout

\begin_layout Standard
Given a time series 
\begin_inset Formula $t_{n}$
\end_inset

,
 and its counter 
\begin_inset Formula $C_{t}$
\end_inset

,
 a function 
\begin_inset Formula $\lambda:\mathbb{R}\to\mathbb{R}$
\end_inset

 will be called the 
\emph on
rate of 
\begin_inset Formula $C_{t}$
\end_inset


\emph default
 if 
\begin_inset Formula $\mathbb{P}\left[C_{t+\delta t}=C_{t}+1\right]=\lambda\left(t\right)dt+o\left(dt\right)$
\end_inset

,
 in the sense that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\lim_{\delta t\to0}\frac{\mathbb{P}\left[C_{t+\delta t}=C_{t}+1\right]}{\lambda\left(t\right)dt}=1
\]

\end_inset


\end_layout

\begin_layout Standard
A time series will be called 
\emph on
ageless
\emph default
 if its counter's rate is a constant function 
\begin_inset Formula $\lambda\left(t\right)\equiv\lambda$
\end_inset

.
\end_layout

\begin_layout Subsection*
Theorem
\end_layout

\begin_layout Quote
The dwell time for the first event in an ageless time series is an exponential random variable.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $t_{n}$
\end_inset

 be an ageless time series,
 and therefore 
\begin_inset Formula $t_{1}=\theta_{0}$
\end_inset

 is the first dwell time as well.
 Define 
\begin_inset Formula $F_{t}\left(s\right)\equiv\mathbb{P}\left[t\leq s\right]$
\end_inset

.
\begin_inset Formula 
\begin{align*}
F_{t}\left(s+dt\right)-F_{t} & =\mathbb{P}\left[t\in(s,s+\delta t]\right]=\left(1-F_{t}\left(s\right)\right)\mathbb{P}\left[C_{t+\delta t}=C_{t}+1\right]\\
 & =\left(1-F_{t}\left(s\right)\right)\left(\lambda\delta t+o\left(\delta t\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And therefore:
\begin_inset Formula 
\[
\frac{dF_{t}}{ds}\left(s\right)=\lambda\left(1-F_{t}\left(s\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The solution to which,
 given that 
\begin_inset Formula $F_{t}\left(0\right)=0$
\end_inset

 since nothing can happen in no time,
 exists and is unique (Poincare).
 It just happens to be 
\begin_inset Formula $F_{t}\left(s\right)=1-e^{-\lambda s}$
\end_inset

.
 You can derive and check for yourself.
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $F$
\end_inset

 is the CDF of the first dwell time,
 
\begin_inset Formula $t$
\end_inset

 is an exponential random variable by law.
 
\begin_inset Formula $\square$
\end_inset


\end_layout

\begin_layout Subsection
Probability Generating Function
\end_layout

\begin_layout Standard
Given a random variable 
\begin_inset Formula $X$
\end_inset

,
 with values in 
\begin_inset Formula $\mathbb{N}$
\end_inset

,
 its 
\emph on
probability generating function
\emph default
 (PGF) is defined by 
\begin_inset Formula $\mathbb{E}\left[x^{X}\right]$
\end_inset

.
 We will denote it as 
\begin_inset Formula $H\left(X\right)$
\end_inset

.
\end_layout

\begin_layout Standard
By definition,
 
\begin_inset Formula $\mathbb{E}\left[x^{X}\right]=\sum p_{n}x^{n}$
\end_inset

.
\end_layout

\begin_layout Standard
The continuous version would be a transform,
 kind of like Fourier analysis.
\end_layout

\begin_layout Subsubsection*
Proposition
\end_layout

\begin_layout Quote
The PGF is an analytic function over the unit circle (
\begin_inset Formula $x<1$
\end_inset

).
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\left|x\right|\leq1$
\end_inset

.
 Thus 
\begin_inset Formula $\left|x^{n}\right|\leq1$
\end_inset

,
 and 
\begin_inset Formula $\left|x^{n}p_{n}\right|\leq p_{n}$
\end_inset

.
\end_layout

\begin_layout Standard
Obviously,
 
\begin_inset Formula $\sum_{n=0}^{k}p_{n}$
\end_inset

 is monotonically increasing and bounded by 1,
 and thus converges (to 1,
 as a probability).
 It is a Majorant for the PGF,
 and therefore the PGF 
\begin_inset Formula $\sum p_{n}x^{n}$
\end_inset

 converges uniformly over the unit circle (Weierstrass M-test).
 
\begin_inset Formula $\square$
\end_inset


\end_layout

\begin_layout Standard
The function is analytic over the unit circle,
 and therefore all its derivatives are defined.
\end_layout

\begin_layout Subsubsection*
Proposition
\end_layout

\begin_layout Quote
If 
\begin_inset Formula $\lim_{x\to1}\frac{d^{n}H}{dx^{n}}\equiv\frac{d^{n}H}{dx^{n}}|_{x=1}$
\end_inset

 exists and is finite,
 then 
\begin_inset Formula $\frac{d^{n}H}{dx^{n}}|_{x=1}=F_{n}\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{d^{n}H}{dx^{n}}|_{x=1}=\lim_{x\to1}\sum p_{n}\prod_{k=0}^{n-1}\left(n-k\right)x^{n-1}=\sum p_{n}\prod_{k=0}^{n-1}\left(n-k\right)\equiv F_{n}\left(X\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Where inserting the derivatives into the sum uses our uniform convergence.
 
\begin_inset Formula $\square$
\end_inset


\end_layout

\begin_layout Standard
Where doesn't this work?
 We play until we hit heads,
 and we get money for 
\begin_inset Formula $2^{n}$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the number of tails we get.
 The probability to get 
\begin_inset Formula $n$
\end_inset

 is 
\begin_inset Formula $p_{n}=2^{-n}$
\end_inset

.
 Thus:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{E}\left[X\right]=\sum p_{n}2^{n}=\sum1=\infty
\]

\end_inset


\end_layout

\begin_layout Standard
And the PGF goes as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H\left(X\right)=\sum2^{-n}x^{n}
\]

\end_inset


\end_layout

\begin_layout Standard
But this has no derivative in 
\begin_inset Formula $x=1$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Observation
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $H\left(x\right)$
\end_inset

 is analytic over the unit circle,
 written as a power series 
\begin_inset Formula $H\left(x\right)=\sum a_{n}x^{n}$
\end_inset

 with 
\begin_inset Formula $a_{n}>0$
\end_inset

 and 
\begin_inset Formula $\sum a_{n}=1$
\end_inset

,
 then there is some random variable such that 
\begin_inset Formula $H$
\end_inset

 is its PGF,
 which is defined by the distribution 
\begin_inset Formula $a_{n}$
\end_inset

.
\end_layout

\begin_layout Paragraph*
Denotation
\end_layout

\begin_layout Standard
We denote the coefficient of 
\begin_inset Formula $x^{n}$
\end_inset

 in a McLauren power series of 
\begin_inset Formula $H$
\end_inset

 as 
\begin_inset Formula $\left[H\left(X\right)\right]_{n}=\sum_{\sum n_{j}=n}\prod_{i=1}^{j}a_{n_{i}}$
\end_inset


\end_layout

\begin_layout Subsubsection*
Corollary
\end_layout

\begin_layout Quote
If 
\begin_inset Formula $n\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $H\left(X\right)$
\end_inset

 is a PGF,
 then 
\begin_inset Formula $H^{n}\left(X\right)$
\end_inset

 is a PGF.
\end_layout

\begin_layout Standard
\begin_inset Formula $H^{n}\left(X\right)$
\end_inset

 is analytic by composition.
 
\begin_inset Formula $H^{n}\left(1\right)=1=\sum b_{n}$
\end_inset

.
 
\begin_inset Formula $b_{n}>0$
\end_inset

 since it is simply given as multiplications of 
\begin_inset Formula $a_{n}$
\end_inset

 which are all positive.
\end_layout

\begin_layout Standard
\begin_inset Formula $H^{n}\left(X\right)\Rightarrow\sum_{i=1}^{n}X_{i}$
\end_inset

 for 
\begin_inset Formula $X_{i}$
\end_inset

 which are i.i.d with 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Section
24.3.2025
\end_layout

\begin_layout Subsection
Some more preparation
\end_layout

\begin_layout Standard
We are almost getting to what a branching process is!
 We dealt with the PGF last time,
 and it's going to take us far.
\end_layout

\begin_layout Standard
Reminder:
 If 
\begin_inset Formula $X:\mathbb{N\to\mathbb{R}}$
\end_inset

 is a random variable,
 we define:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{X}\left(x\right)=\mathbb{E}\left[x^{X}\right]=\sum_{n}p_{n}x^{n}
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
Proposition
\end_layout

\begin_layout Quote
If 
\begin_inset Formula $X,Y$
\end_inset

 are independent,
 then 
\begin_inset Formula 
\[
H_{X+Y}\left(x\right)=H_{x}(x)H_{Y}(x)
\]

\end_inset


\end_layout

\begin_layout Standard
We denote 
\begin_inset Formula $a_{n}=\mathbb{P}\left[X=n\right],b_{n}=\mathbb{P}\left[Y=n\right]$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{P}\left[x+y=n\right]=\sum_{k=0}^{n}\mathbb{P}\left[X=k\right]\mathbb{P}\left[Y=n-k\right]=\sum_{k=0}^{n}a_{k}b_{n-k}=\left[H_{X}\left(x\right)H_{Y}\left(x\right)\right]_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\square$
\end_inset


\end_layout

\begin_layout Standard
A different proof,
 which is even shorter,
 is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{X+Y}\left(x\right)=\mathbb{E}\left[x^{X+Y}\right]=\mathbb{E}\left[x^{X}x^{Y}\right]=\mathbb{E}\left[x^{X}\right]\mathbb{E}\left[x^{Y}\right]=H_{X}\left(x\right)H_{Y}\left(x\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\square$
\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $X$
\end_inset

 is an exponential random variable with parameter 
\begin_inset Formula $\lambda$
\end_inset


\begin_inset Formula 
\[
\mathbb{P}\left[x\leq t\right]=\int_{0}^{t}\lambda e^{-\lambda s}ds
\]

\end_inset


\end_layout

\begin_layout Standard
and 
\begin_inset Formula $Y$
\end_inset

 is an independent exponential random variable with parameter 
\begin_inset Formula $\Lambda$
\end_inset

,
 then:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min\left(X,Y\right)\sim Exp\left(\lambda+\Lambda\right)
\]

\end_inset


\end_layout

\begin_layout Standard
This can be proven graphically.
 Take a square of in an 
\begin_inset Formula $X,Y$
\end_inset

 plane,
 where they are both less than some 
\begin_inset Formula $t$
\end_inset

.
 The space that excludes the opposite square in that half place across from 
\begin_inset Formula $\left(t,t\right)$
\end_inset

 is where the minimum is under 
\begin_inset Formula $t$
\end_inset

.
 This can be seen in the following graphic image of the integration range:
\begin_inset Foot
status open

\begin_layout Plain Layout
Thanks to Aviv Barnea for the tikz code for this image
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}
\end_layout

\begin_layout Plain Layout


\backslash
fill[blue!20] (2,2) rectangle (10,10);
\end_layout

\begin_layout Plain Layout

    
\backslash
draw (0,
 0) -- (10,0);
\end_layout

\begin_layout Plain Layout

    
\backslash
draw (0,
 0) -- (0,
 10);
\end_layout

\begin_layout Plain Layout

    
\backslash
draw[blue!80,
 dashed] (0,
 2) -- (10,
 2);
\end_layout

\begin_layout Plain Layout

    
\backslash
draw[blue!80,
 dashed] (2,
 0) -- (2,
 10);
\end_layout

\begin_layout Plain Layout

    
\backslash
node[below] at (2,2) {$(t,t)$};
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Therefore:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{P}\left[\min\left(X,Y\right)\leq t\right]=1-\iint_{\mathbb{R}^{2+}\backslash D}\lambda\Lambda e^{-\lambda s}e^{-\Lambda\omega}dsd\omega=1-\int_{t}^{\infty}ds\int_{t}^{\infty}\lambda\Lambda e^{-\lambda s}e^{-\Lambda\omega}d\omega=1-e^{-\left(\lambda+\Lambda\right)t}
\]

\end_inset


\end_layout

\begin_layout Standard
And therefore its CDF is that of a random exponential variable with parameter 
\begin_inset Formula $\lambda+\Lambda$
\end_inset

.
\begin_inset Formula $\square$
\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X,Y$
\end_inset

 be random variables.
 The law of dependent probability is just:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{E}\left[X|Y=y\right]=\sum\mathbb{P}\left[X=n|Y=y\right]n
\]

\end_inset


\end_layout

\begin_layout Standard
Mishpat Hahachlaka (the law of total expectation) says that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{E}\left[\mathbb{E}\left[X|Y\right]\right]=\mathbb{E}\left[X\right]
\]

\end_inset


\end_layout

\begin_layout Standard
This is a form of Fubini's theorem,
 obviously,
 that we can take the integral in whichever order we like.
\end_layout

\begin_layout Standard
The law of total variance says that:
\end_layout

\begin_layout Standard
We need of course for the expectation and variance to both exist,
 for this to make sense.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V\left[X\right]=\mathbb{E}\left[V\left[X|Y\right]\right]+V\left[\mathbb{E}\left[X|Y\right]\right]
\]

\end_inset


\end_layout

\begin_layout Standard
And NOW,
 we can finally get to branching processes,
 but we'll start with Galton-Watson Trees,
 even though we won't actually use them much as a formalism.
\end_layout

\begin_layout Subsection
Galton-Watson Trees
\end_layout

\begin_layout Standard
They were an economist and a mathmatician who tried to figure out why the number of different noble family names was dropping off.
\end_layout

\begin_layout Standard
We follow a series 
\begin_inset Formula $G_{n}$
\end_inset

 of trees in the graph theory sense 
\begin_inset Formula $G_{n}=\left(P_{n},V_{n}\right)$
\end_inset

,
 and a color function 
\begin_inset Formula $f_{n}:P_{n}\to\left\{ 0,1\right\} $
\end_inset

.
 1 marks a 
\emph on
living
\emph default
 node,
 and 0 a dead node We always take 
\begin_inset Formula $G_{1}=\left(\left\{ p_{0}\right\} ,\phi\right)$
\end_inset

,
 
\begin_inset Formula $f_{1}\left(p_{0}\right)=1$
\end_inset

.
 At any stage in the series we sample the number of new nodes and edges we will introduce for each node in 
\begin_inset Formula $f_{n}^{-1}\left(1\right)$
\end_inset

,
 and we add those as leafs.
 We set 
\begin_inset Formula $f_{n}^{-1}\left(1\right)=\left\{ p|p\in P_{n}\backslash P_{n-1}\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
We still need to define how we sample the new progeny,
 of course,
 for this to be complete.
\end_layout

\begin_layout Standard
The series 
\begin_inset Formula $G_{n}$
\end_inset

 will be defined by the series of distributions 
\begin_inset Formula $\left\{ \nu_{n}\left(k\right)\right\} $
\end_inset

 where 
\begin_inset Formula $\nu_{n}\left(k\right)=\mathbb{P}\left[\text{Getting }k\text{ progeny for any live node in }G_{n}\right]$
\end_inset

.
 We can have this distribution depend on the size of 
\begin_inset Formula $f_{n}^{-1}\left(1\right)$
\end_inset

.
 We won't consider non-Markovian cases in this course.
\end_layout

\begin_layout Standard
We move on to a modern definition that is more common.
\end_layout

\begin_layout Subsection
Branching Processes (Finally!)
\end_layout

\begin_layout Standard
A random process 
\begin_inset Formula $X_{n}$
\end_inset

 will be called a 
\emph on
Discrete Branching Process
\emph default
 if there exist random variables 
\begin_inset Formula $\xi_{i,n}$
\end_inset

 that give values in 
\begin_inset Formula $\mathbb{N}$
\end_inset

,
 which are all i.i.d,
 such that:
\begin_inset Formula 
\[
X_{n+1}=\sum_{i=1}^{X_{n}}\xi_{i,n}
\]

\end_inset


\end_layout

\begin_layout Standard
A random process 
\begin_inset Formula $X_{n}$
\end_inset

 will be called a 
\emph on
Branching Process with Immigration
\emph default
 if it follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{n+1}=\sum_{i=1}^{X_{n}}\xi_{i,n}+I_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $I_{n}$
\end_inset

 is some random variable with values in 
\begin_inset Formula $\mathbb{Z}$
\end_inset

,
 but limited to not causing a negative 
\begin_inset Formula $X_{n+1}$
\end_inset

,
 to avoid summing over negative populations.
\end_layout

\begin_layout Standard
A discrete branching process will be called 
\emph on
stationary
\emph default
 if 
\begin_inset Formula $\xi_{i,n}$
\end_inset

 are i.i.d for all 
\begin_inset Formula $n$
\end_inset

,
 so they can be written as 
\begin_inset Formula $\xi_{i,n}\sim\xi$
\end_inset

.
 Specifically,
 they do not depend on the population size 
\begin_inset Formula $X_{n}$
\end_inset

,
 nor 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Standard
The dynamics is then dependent only on a single law 
\begin_inset Formula $\xi$
\end_inset

,
 with a probability vector 
\begin_inset Formula $\mathbb{P}\left[\xi=n\right]=p_{n}$
\end_inset

.
 If we have immigration,
 we also demand 
\begin_inset Formula $I_{n}\sim I$
\end_inset

 be i.i.d.
\end_layout

\begin_layout Subsection*
The Microbe Problem
\end_layout

\begin_layout Standard
We start with an initial population 
\begin_inset Formula $n_{0}=1$
\end_inset

 (i.e.
 
\begin_inset Formula $X_{0}=1$
\end_inset

 with probability 1).
 With probability 
\begin_inset Formula $p$
\end_inset

 it will mitosis (become 2),
 with 
\begin_inset Formula $1-p$
\end_inset

 it will die.
\end_layout

\begin_layout Standard
What is the distribution of the number of microbes in generation 
\begin_inset Formula $n$
\end_inset

?
\end_layout

\begin_layout Standard
We define 
\begin_inset Formula $Q\left(z\right)=\left(1-p\right)+pz^{2}$
\end_inset

,
 which is the PGF for the number of progeny of a single microbe.
 We denote the compunding of a function 
\begin_inset Formula $Q$
\end_inset

 on itself 
\begin_inset Formula $k$
\end_inset

 times as 
\begin_inset Formula $Q^{\left[k\right]}$
\end_inset

.
\end_layout

\begin_layout Standard
We denote 
\begin_inset Formula $H_{n}\left(x\right)$
\end_inset

 as the PGF for the 
\begin_inset Formula $n$
\end_inset

-th generation.
 
\begin_inset Formula $H_{0}\left(x\right)=x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{n}\left(x\right)=\sum a_{n,k}x^{k}
\]

\end_inset


\end_layout

\begin_layout Standard
By definition.
\end_layout

\begin_layout Standard
We recursively know that,
 by the law of total probability based on what happened in generation 1:
\begin_inset Formula 
\begin{align*}
a_{n,k} & =\mathbb{P}\left[k\text{ in generation }n|\text{mitosis}\right]p+\mathbb{P}\left[k\text{ in generation }n|death\right]\left(1-p\right)=\\
 & =p\sum_{j=0}^{k}a_{n-1,j}a_{n-1,k-j}+\delta_{0,k}\left(1-p\right)\\
 & =p\left[H_{n-1}^{2}\left(x\right)\right]_{k}+\delta_{0,k}\left(1-p\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We multiply by 
\begin_inset Formula $x^{k}$
\end_inset

 and sum over them:
\begin_inset Formula 
\[
H_{n}\left(x\right)=pH_{n-1}^{2}\left(x\right)+\left(1-p\right)=Q\left(H_{n-1}\left(x\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
since in the first generation we either die off to 0 and stay there,
 or we have two individuals that need to compound in 
\begin_inset Formula $n-1$
\end_inset

 generations.
\begin_inset Formula 
\[
H_{n}\left(x\right)=Q^{\left[n\right]}\left(x\right)
\]

\end_inset


\end_layout

\begin_layout Standard
And therefore:
\begin_inset Formula 
\begin{align*}
H_{0} & =x\\
H_{1} & =px^{2}+1-p\\
H_{2} & =p^{3}x^{4}+2p^{2}\left(1-p\right)x^{2}+p\left(1-p\right)^{2}+\left(1-p\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And so on and so forth.
\end_layout

\begin_layout Standard
What is the expectation and variance?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M_{1}\left(n\right)=\mathbb{E}\left[X_{n}\right]=\frac{dH_{n}}{dx}|_{1}=\frac{dH_{n}}{dH_{n-1}}\frac{dH_{n-1}}{dx}|_{1}=2pH_{n-1}|_{1}\mathbb{E}\left[H_{n-1}\right]=2pM_{1}\left(n-1\right)=\left(2p\right)^{n}1=\left(2p\right)^{n}
\]

\end_inset


\end_layout

\begin_layout Standard
We can divide this into 3 cases,
 based on the ordering of 
\begin_inset Formula $2p$
\end_inset

 and 
\begin_inset Formula $1$
\end_inset

,
 into subcritical,
 critical and supercritical regimes 
\begin_inset Formula $M_{1}\left(n\right)\underset{n\to\infty}{\to}\begin{cases}
0 & 2p<1\\
n_{0} & 2p=1\\
\infty & 2p>1
\end{cases}$
\end_inset


\end_layout

\begin_layout Standard
Similarly,
 we can do the variance:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
F_{2}\left(n\right) & =V\left(X_{n}\right)=\frac{d^{2}H_{n}}{dx^{2}}|_{1}=\left[Q''\left(H_{n-1}\right)\left(H_{n-1}^{'}\right)^{2}+Q'\left(H_{n-1}\right)H_{n-1}^{''}\right]_{1}=2pM_{1}^{2}\left(n-1\right)+2pF_{2}\left(n-1\right)\\
 & =2p\left(2p\right)^{2\left(n-1\right)}+2pF_{2}\left(n-1\right)=\left(2p\right)^{2n-1}+2pF_{2}\left(n-1\right)=2pF_{2}\left(n-1\right)+\frac{1}{2p}\left(\left(2p\right)^{2}\right)^{n}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This is an inhomogeneous recursion relation of the form:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
F_{2}\left(n\right)=F_{2,h}\left(n\right)+F_{2,p}\left(n\right)
\]

\end_inset


\begin_inset Formula 
\[
F_{2,h}\left(n\right)=2pF_{2,h}\left(n-1\right)\Rightarrow F_{2,h}=C\left(2p\right)^{n}
\]

\end_inset


\end_layout

\begin_layout Standard
Reminder,
 generally the particular solution of the equation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
a_{n}=qa_{n-1}+\gamma^{n}A
\]

\end_inset


\end_layout

\begin_layout Standard
is of the form 
\begin_inset Formula $a_{n,p}=B\gamma^{n}$
\end_inset

 unless 
\begin_inset Formula $\gamma=q$
\end_inset

,
 in which case 
\begin_inset Formula $a_{n,p}=Bn\gamma^{n}$
\end_inset

.
 Thus,
 for 
\begin_inset Formula $2p\neq1$
\end_inset

,
 we get:
\begin_inset Formula 
\[
F_{2,p}\left(n\right)=B\left[\left(2p\right)^{2}\right]^{n}
\]

\end_inset


\end_layout

\begin_layout Standard
When 
\begin_inset Formula $2p=1$
\end_inset

,
 
\begin_inset Formula $2p=\left(2p\right)^{2}$
\end_inset

 and we need the other particular solution,
 and we get:
\begin_inset Formula 
\[
F_{2,p}\left(n\right)=Bn
\]

\end_inset


\end_layout

\begin_layout Standard
Which results in a particular solution that is solved to:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Bn=B\left(n-1\right)+1\Rightarrow B=1
\]

\end_inset


\end_layout

\begin_layout Standard
And therefore:
\begin_inset Formula 
\[
F_{2}\left(n\right)=C+n
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $C$
\end_inset

 comes from boundary conditions,
 so for our case 
\begin_inset Formula $C=0$
\end_inset

,
 because 
\begin_inset Formula $F_{2}\left(0\right)=\mathbb{E}\left[X_{n}\left(X_{n}-1\right)\right]=\mathbb{E}\left[X_{n}^{2}\right]-1=0$
\end_inset

.
\begin_inset Formula 
\[
F_{2}\left(n\right)=n
\]

\end_inset


\end_layout

\begin_layout Standard
The variance is simply:
\begin_inset Formula 
\[
V\left(X_{n}\right)=\mathbb{E}\left[X_{n}^{2}\right]-\mathbb{E}^{2}\left[X_{n}\right]=n
\]

\end_inset


\end_layout

\begin_layout Standard
This is the irregular case where the mean does not actually describe a likely case of the population.
 There will be many populations that grow and multiple populations that die off,
 and they cancel our 
\series bold
on average
\series default
,
 but the behavior of the population is a cone that starts at 
\begin_inset Formula $n_{0}$
\end_inset

 and moves up or down with a random walk.
\end_layout

\begin_layout Subsection
Generalized Microbe Problem
\end_layout

\begin_layout Standard
Now,
 when we mitosis,
 we have a probability distribution for our progeny,
 which we will name 
\emph on
multiplicity
\emph default
,
 and mark it 
\begin_inset Formula $\left\{ p_{\nu}\left(n\right)\right\} _{n=0}^{\infty}$
\end_inset

.
 We assume that all its moments exist and are finite.
 Death is now a part of life,
 such that mitosis to 0 is death.
\end_layout

\begin_layout Standard
We still have,
 in the exact same way:
\begin_inset Formula 
\[
a_{n}\left(k\right)=\delta_{k,0}p_{\nu}\left(0\right)+\sum_{l=0}^{\infty}p_{\nu}\left(l\right)\sum_{\sum_{j=1}^{l}k_{j}=k}\prod_{j=0}^{l}a_{n-1}\left(k_{j}\right)
\]

\end_inset


\begin_inset Formula 
\[
a_{n}\left(k\right)=\delta_{k,0}p_{\nu}\left(0\right)+\sum_{l=0}^{\infty}p_{\nu}\left(l\right)\left[H_{n-1}^{l}\right]_{k}
\]

\end_inset


\begin_inset Formula 
\[
H_{n}\left(x\right)=Q^{\left[n\right]}\left(x\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $Q$
\end_inset

 is the new multiplicity PGF.
\end_layout

\begin_layout Standard
We still get,
 because we never used anything comples:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M_{1}\left(n\right)=\left(\sum\nu p_{\nu}\right)M_{1}\left(n-1\right)\equiv\bar{\nu}M_{1}\left(n-1\right)=\overline{\nu}^{n}n_{0}
\]

\end_inset


\end_layout

\begin_layout Standard
So we still get the same criticality behavior,
 based on the sign of 
\begin_inset Formula $\overline{\nu}-1$
\end_inset

.
\end_layout

\begin_layout Paragraph
Definition
\end_layout

\begin_layout Standard
We call a stationary branching process 
\emph on
subcritical,
 critical and supercritical
\emph default
 based on 
\begin_inset Formula $\overline{\nu}$
\end_inset

 being less than,
 equal or greater than 1.
\end_layout

\begin_layout Section
31.3.2025
\end_layout

\begin_layout Quote
I did not attend this lecture,
 but rather wrote this off of Tomer's notes.
 Thanks,
 Tomer!
\end_layout

\begin_layout Standard
Last time we studied stationary branching processes,
 which are uniquely defined by their multiplicity 
\begin_inset Formula $\left\{ p_{\nu}\left(n\right)\right\} _{n=0}^{\infty}$
\end_inset

.
 We saw that for an initial population 
\begin_inset Formula $n_{0}=1$
\end_inset

,
 the PGF for the population at generation 
\begin_inset Formula $n$
\end_inset

 was given recursively by:
\begin_inset Formula 
\[
g_{n+1}\left(x\right)=Q\left(g_{n}\left(x\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $Q$
\end_inset

 is the PGF for the multiplicity.
\end_layout

\begin_layout Standard
We also saw that 
\begin_inset Formula $\mathbb{E}\left[X_{n}\right]=\overline{\nu}^{n}$
\end_inset

.
\end_layout

\begin_layout Standard
Let us now compute the variance.
 For the first moment we had:
\begin_inset Formula 
\[
\frac{dg_{n+1}}{dx}\left(x\right)=\frac{d}{dx}\left[\sum p_{\nu,k}g_{n}^{k}\left(x\right)\right]=Q'\left(g_{n}\left(x\right)\right)g_{n}'\left(x\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\[
\frac{d^{2}g_{n+1}}{dx^{2}}\left(x\right)=\frac{d}{dx}\left[Q'\left(g_{n}\left(x\right)\right)g_{n}'\left(x\right)\right]=Q''\left(g_{n}\left(x\right)\right)\left[g_{n}'\left(x\right)\right]^{2}+Q'\left(g_{n}\left(x\right)\right)g_{n}''\left(x\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Setting 
\begin_inset Formula $x=1$
\end_inset

 we get:
\begin_inset Formula 
\[
F_{2}\left(n+1\right)=\overline{\nu\left(\nu-1\right)}M_{1}^{2}\left(n\right)+\overline{\nu}F_{2}\left(n\right)=\overline{\nu\left(\nu-1\right)}\overline{\nu}^{2n}+\overline{\nu}F_{2}\left(n\right)
\]

\end_inset


\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\overline{\nu}\neq1$
\end_inset

 we solve the homogeneous and particular solutions:
\begin_inset Formula 
\[
a_{h}\left(n\right)=\overline{\nu}^{n},\quad a_{p}\left(n\right)=A\overline{\nu}^{2n}
\]

\end_inset


\end_layout

\begin_layout Standard
Solving for 
\begin_inset Formula $A$
\end_inset

 we get:
\begin_inset Formula 
\[
A\overline{\nu}^{2n+2}=A\overline{\nu}^{2n}+\overline{\nu\left(\nu-1\right)}\overline{\nu}^{2n}
\]

\end_inset


\end_layout

\begin_layout Standard
And thus:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
A\overline{\nu}^{2}=A+\overline{\nu\left(\nu-1\right)}
\]

\end_inset


\begin_inset Formula 
\[
A=\frac{\overline{\nu\left(\nu-1\right)}}{\overline{\nu}^{2}-1}
\]

\end_inset


\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\overline{\nu}=1$
\end_inset

,
 
\begin_inset Formula $M_{1}\left(n\right)=1$
\end_inset

 and so:
\begin_inset Formula 
\[
F_{2}\left(n+1\right)=F_{2}\left(n\right)+\overline{\nu\left(\nu-1\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
Again:
\begin_inset Formula 
\[
a_{h}\left(n\right)=1,\quad a_{p}\left(n\right)=An
\]

\end_inset


\end_layout

\begin_layout Standard
which is solved for 
\begin_inset Formula $A$
\end_inset

 similarly to be:
\begin_inset Formula 
\[
A=\overline{\nu\left(\nu-1\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
Which results in a simple form for 
\begin_inset Formula $F_{2}$
\end_inset

:
\begin_inset Formula 
\[
F_{2}\left(n\right)=C+n\overline{\nu\left(\nu-1\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
Which,
 when we translate to a variance would be:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V\left[X_{n}\right]=\mathbb{E}\left[X_{n}^{2}\right]-\mathbb{E}^{2}\left[X_{n}\right]=\mathbb{E}\left[X_{n}^{2}\right]-1
\]

\end_inset


\begin_inset Formula 
\[
F_{2}\left(n\right)=\mathbb{E}\left[X_{n}\left(X_{n}-1\right)\right]=\mathbb{E}\left[X_{n}^{2}\right]-\mathbb{E}\left[X_{n}\right]=\mathbb{E}\left[X_{n}^{2}\right]-1=V\left[X_{n}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
And thus,
 since
\begin_inset Formula 
\[
V\left[X_{0}\right]=0\Rightarrow C=0
\]

\end_inset


\end_layout

\begin_layout Standard
So finally
\begin_inset Formula 
\[
V\left[X_{n}\right]=n\overline{\nu\left(\nu-1\right)}=n\left(\overline{\nu^{2}}-1\right)=n\sigma^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is the variance of the multiplicity.
\end_layout

\begin_layout Subsection
Example
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $p_{\nu,i}=\frac{1}{3}\sum_{i=0}^{2}\delta_{ij}p_{\nu,j}$
\end_inset

.
 What is the criticality of the system?
 What is 
\begin_inset Formula $g_{2}$
\end_inset

 if 
\begin_inset Formula $n_{0}=1$
\end_inset

?
 What is 
\begin_inset Formula $g_{2}$
\end_inset

 assuming 
\begin_inset Formula $n_{0}=1000$
\end_inset

?
\end_layout

\begin_layout Subsubsection
Criticality
\begin_inset Formula 
\[
\overline{\nu}=\frac{1}{3}+2\frac{1}{3}=1
\]

\end_inset


\end_layout

\begin_layout Standard
So it is critical.
\end_layout

\begin_layout Subsubsection
\begin_inset Formula $g_{2},n_{0}=1$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g_{0}\left(x\right)=x
\]

\end_inset


\begin_inset Formula 
\[
g_{1}\left(x\right)=Q\left(g_{0}\left(x\right)\right)=Q\left(x\right)=\frac{1}{3}\left(1+x+x^{2}\right)
\]

\end_inset


\begin_inset Formula 
\[
g_{2}\left(x\right)=\frac{1}{3}\left(1+g_{1}\left(x\right)+g_{1}^{2}\left(x\right)\right)=\frac{1}{3}\left(1+\frac{1}{3}\left(1+x+x^{2}\right)+\frac{1}{9}\left(1+x+x^{2}\right)^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Which you can simplify,
 if you must.
\end_layout

\begin_layout Subsubsection
\begin_inset Formula $g_{2},n_{0}=1000$
\end_inset


\end_layout

\begin_layout Standard
We know that if 
\begin_inset Formula $X,Y$
\end_inset

 are i.i.d.
 random variables,
 
\begin_inset Formula $g_{X+Y}=g_{X}g_{Y}$
\end_inset

,
 and so 
\begin_inset Formula $g_{\sum X_{i}}=\prod g_{X_{i}}$
\end_inset

 in general,
 if 
\begin_inset Formula $\left\{ X_{i}\right\} $
\end_inset

 are i.i.d.
 random variables.
 More specifically:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g_{\sum X_{i}}=g_{X_{1}}^{n_{0}}
\]

\end_inset


\end_layout

\begin_layout Standard
And thus:
\begin_inset Formula 
\[
g_{n_{0}=1000}=\left[\frac{1}{3}\left(1+\frac{1}{3}\left(1+x+x^{2}\right)+\frac{1}{9}\left(1+x+x^{2}\right)^{2}\right)\right]^{1000}
\]

\end_inset


\end_layout

\begin_layout Subsection
Stationary branching processes with immigration
\begin_inset Formula 
\[
X_{n+1}=\sum_{i=1}^{X_{n}}\xi_{i,n}+I_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
We denote the multiplicity of 
\begin_inset Formula $\xi$
\end_inset

 as 
\begin_inset Formula $p_{\nu}$
\end_inset

,
 as before,
 and that of 
\begin_inset Formula $I$
\end_inset

 as 
\begin_inset Formula $p_{i}$
\end_inset

.
 The PGFs are denoted as 
\begin_inset Formula $Q,I$
\end_inset

 respectively.
 We denote the moments of 
\begin_inset Formula $I$
\end_inset

 with symbols such as 
\begin_inset Formula $\overline{\nu}_{i}$
\end_inset

 for the first.
\end_layout

\begin_layout Subsection*
Theorem
\end_layout

\begin_layout Quote
Denote 
\begin_inset Formula $g_{n}$
\end_inset

 as the PGF of the 
\begin_inset Formula $n$
\end_inset

-th generation,
 given 
\begin_inset Formula $X_{0}=1$
\end_inset

.
 Therefore:
\begin_inset Formula 
\[
g_{n+1}\left(x\right)=g_{n}\left(Q\left(x\right)\right)I\left(x\right)
\]

\end_inset


\end_layout

\begin_layout Paragraph
Proof
\end_layout

\begin_layout Standard
By definition:
\begin_inset Formula 
\[
g_{n+1}\left(x\right)=\mathbb{E}\left[x^{X_{n+1}}\right]=\mathbb{E}\left[x^{\sum_{i=1}^{X_{n}}\xi_{i,n}+I_{n}}\right]=\mathbb{E}\left[x^{\sum_{i=1}^{X_{n}}\xi_{i,n}}x^{I_{n}}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
using the law of total probability:
\begin_inset Formula 
\begin{align*}
g_{n+1}\left(x\right) & =\mathbb{E}\left[\mathbb{E}\left[x^{\sum_{i=1}^{X_{n}}\xi_{i,n}}x^{I_{n}}|X_{n}\right]\right]=\mathbb{E}\left[\mathbb{E}\left[x^{\sum_{i=1}^{X_{n}}\xi_{i,n}}\right]|X_{n}\right]\mathbb{E}\left[x^{I_{n}}\right]\\
 & =\mathbb{E}\left[Q^{X_{n}}\left(x\right)\right]\mathbb{E}\left[x^{I_{n}}\right]=g_{n}\left(Q\left(x\right)\right)I\left(x\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\square$
\end_inset


\end_layout

\begin_layout Subsubsection*
Corollary
\end_layout

\begin_layout Quote
\begin_inset Formula 
\[
M_{1}\left(n\right)=\begin{cases}
\left(1-\frac{\overline{\nu_{i}}}{1-\overline{\nu}}\right)\overline{\nu}^{n}+\frac{\overline{\nu_{i}}}{1-\overline{\nu}} & \overline{\nu}\neq1\\
1+n\overline{\nu_{i}} & \nu=1
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Quote
This behavior is expected:
 Exponential growth or decay,
 and a linear growth for critical cases,
 as the system's expectation meets the deterministic case.
 The exponential case is also always larger than a population with no immigration.
\end_layout

\begin_layout Paragraph
Proof
\begin_inset Formula 
\[
\frac{dg_{n+1}}{dx}=g_{n}'\left(Q\right)Q'I+g_{n}\left(Q\right)I'
\]

\end_inset


\end_layout

\begin_layout Standard
Set 
\begin_inset Formula $x=1$
\end_inset

 and we get:
\begin_inset Formula 
\[
M_{1}\left(n+1\right)=M_{1}\left(n\right)\overline{\nu}+\overline{\nu_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\overline{\nu}\neq1$
\end_inset

,
 we solve the homogeneous and particular parts.
 The homogeneous is still 
\begin_inset Formula $C\overline{\nu}^{n}$
\end_inset

.
 The particular solution we assume constant at 
\begin_inset Formula $A$
\end_inset

,
 which solves for 
\begin_inset Formula $A=\frac{\overline{\nu_{i}}}{1-\overline{\nu}}$
\end_inset

.
 We use the initial condition to be 
\begin_inset Formula $1$
\end_inset

,
 to get the solution above.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\overline{\nu}=1$
\end_inset

,
 
\begin_inset Formula $M_{1}\left(n+1\right)=M_{1}\left(n\right)+\overline{\nu_{i}}$
\end_inset

,
 so the homogeneous solution is constant this time around.
 The particular solution we assume to be 
\begin_inset Formula $An$
\end_inset

,
 which solves as 
\begin_inset Formula $A=\overline{\nu_{i}}$
\end_inset

.
 With the initial condition,
 we get the solution above.
\end_layout

\begin_layout Standard
\begin_inset Formula $\square$
\end_inset


\end_layout

\begin_layout Section
7.4.2025
\end_layout

\begin_layout Subsection
The original Galton-Watson paper
\end_layout

\begin_layout Standard
Today we will finish our discussion on discrete branching processes.
 We will discuss the original Galton-Watson paper.
\end_layout

\begin_layout Standard
Let a discrete,
 stationary branching process 
\begin_inset Formula $X_{n}$
\end_inset

,
 with multiplicity 
\begin_inset Formula $\left\{ p_{\nu}\right\} _{\nu=0}^{\infty}$
\end_inset

.
 The extinction probability,
 
\begin_inset Formula $\mathbb{P}\left[X_{n}=0\right]$
\end_inset

,
 we denote as 
\begin_inset Formula $p_{n}\left(0\right)$
\end_inset

,
 just to confuse you with all these 
\begin_inset Formula $p$
\end_inset

s.
\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $p_{n}\left(0\right)\leq1$
\end_inset

,
 as a probability,
 and with no immigration it is a sink case,
 so 
\begin_inset Formula $p_{n}\left(0\right)\leq p_{n+1}\left(0\right)$
\end_inset

.
 Therefore as a bounded series that grows monotonically,
 its limit exists and is finite.
 We denote 
\begin_inset Formula $\lim_{n\to\infty}p_{n}\left(0\right)\equiv p_{\infty}\left(0\right)$
\end_inset

.
\end_layout

\begin_layout Standard
We already saw that the PGF 
\begin_inset Formula $H_{n+1}=Q\left(H_{n}\right)$
\end_inset

.
 Since 
\begin_inset Formula $H_{n}\left(0\right)=p_{n}\left(0\right)$
\end_inset

,
 we can get a recursion relation for 
\begin_inset Formula $H_{n}\left(0\right)$
\end_inset

:
\begin_inset Formula 
\[
p_{n+1}\left(0\right)=Q\left(p_{n}\left(0\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Therefore,
 since the limit exists:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p_{\infty}\left(0\right)=Q\left(p_{\infty}\left(0\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
We want the roots of 
\begin_inset Formula $q\left(x\right)=Q\left(x\right)-x$
\end_inset

 in 
\begin_inset Formula $\left[0,1\right]$
\end_inset

.
 Notice that 
\begin_inset Formula $1$
\end_inset

 is always a solution to this equation,
 because 
\begin_inset Formula $Q$
\end_inset

 is a PGF.
 At 
\begin_inset Formula $x=0$
\end_inset

 this is always non-negative.
\end_layout

\begin_layout Standard
We derive 
\begin_inset Formula $q$
\end_inset

:
\begin_inset Formula 
\[
\frac{dq}{dx}=\left(\sum_{n}p_{\nu,n}nx^{n-1}\right)-1\leq\left(\sum_{n}p_{\nu,n}n\right)-1=\overline{\nu}-1
\]

\end_inset


\end_layout

\begin_layout Standard
This gives us two cases:
\end_layout

\begin_layout Enumerate
Supercritical system,
 
\begin_inset Formula $\overline{\nu}>1$
\end_inset

,
 there can be additional roots in 
\begin_inset Formula $\left[0,1\right]$
\end_inset

 except 
\begin_inset Formula $1$
\end_inset

.
 Basically,
 we don't actually help in this case directly.
\end_layout

\begin_layout Enumerate
Critical or subcritical system,
 
\begin_inset Formula $\overline{\nu}\leq1$
\end_inset

,
 there can only be one root,
 because the function is strictly monotonically decreasing.
 Since 
\begin_inset Formula $p_{\infty}\left(0\right)=1$
\end_inset

 is a root,
 it is the 
\emph on
only
\emph default
 root.
 
\end_layout

\begin_layout Standard
With a population of size 1,
 therefore if you are not supercritical,
 you will go extinct with probability 1 at some point.
 What happens if you start with a population of size 
\begin_inset Formula $n_{0}$
\end_inset

?
 The PGF is simply the product of all contributions,
 so the extinction probability is just 
\begin_inset Formula $p_{\infty}^{n_{0}}\left(0\right)$
\end_inset

.
 If you are critical,
 remember that 
\begin_inset Formula $\mathbb{E}\left[X_{n}\right]=n_{0}$
\end_inset

,
 and by definition 
\begin_inset Formula $\lim_{n\to\infty}\mathbb{E}\left[X_{n}\right]=n_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
This is confusing,
 because we don't actually meet the expectation.
 What really happens is that many chains will die out,
 but some of them will be very large.
 The fraction of surviving initial trees will decay,
 even if the population is actually stable,
 because the population will be dominated by the few lucky trees who survive.
\end_layout

\begin_layout Standard
And that's the Galton-Watson result from their paper.
\end_layout

\begin_layout Subsection
Continuous Branching Processes
\end_layout

\begin_layout Standard
We are considering a discrete population in continuous time.
 So far we used the generation system to define our dynamics.
 We need to move on to dwell times that we previously mentioned.
\end_layout

\begin_layout Standard
Now,
 if we take our Galton-Watson tree,
 we don't define the next generation for all items simultaneously.
 Instead,
 each individual has its own dwell time until it becomes inactive and creates progeny.
 The problem we are going to encounter is that we want the dwell time and multiplicity of individuals to maybe depend on the current population.
 Thus,
 we would need to somehow update the dwell times of individuals and their multiplicity whenever another individual does anything.
\end_layout

\begin_layout Standard
Let's try some naive ways to do it:
\begin_inset Formula 
\[
X_{t+dt}=\sum_{i=1}^{X_{t}}\xi_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
The problem is that we need to somehow make sure there are no additional contributions from progeny born in 
\begin_inset Formula $\left[t,t+dt\right]$
\end_inset

.
 This will fail as a definition.
\end_layout

\begin_layout Standard
We start with the generation definition of 
\begin_inset Formula $X_{n+1}=\sum_{i=1}^{X_{n}}\xi_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
We can define 
\begin_inset Formula $Y_{t}=X_{n_{t}}$
\end_inset

 where 
\begin_inset Formula $n_{t}$
\end_inset

 is a counter of a time series.
 However,
 we want the rate of events to increase with increasing population size,
 so while this definition works,
 it is rather unworkable for most applications.
 So that's too naive too.
\end_layout

\begin_layout Paragraph*
The actual definition we are going to use takes a lot of mathematical proofs to do,
 and we're not going to do all of them.
 We are going to wave our hands a bit,
 bear with me.
 If you want to know the details,
 read the book,
 I will put a link here.
\end_layout

\begin_layout Standard
Given two random processes,
 we want to define a stochastic integral.
 Let 
\begin_inset Formula $X_{t},w_{t}$
\end_inset

 be random processes,
 and we want 
\begin_inset Formula $\int_{a}^{t}X_{s}dw_{s}$
\end_inset

.
\end_layout

\begin_layout Standard
Let a partition 
\begin_inset Formula $S=\left\{ t_{i}\right\} $
\end_inset

 of 
\begin_inset Formula $\left[a,t\right]$
\end_inset

.
 We define 
\begin_inset Formula $ind\left(S\right)=\max\left\{ t_{i+1}-t_{i}\right\} $
\end_inset

.
 We take the Riemann sum 
\begin_inset Formula $I^{I}\left(S\right)=\sum_{i=1}^{n-1}X_{t_{i}}\left(w_{t_{i+1}}-w_{t_{i}}\right)$
\end_inset

.
 This is a random variable,
 and it would also differ if we take 
\begin_inset Formula $X_{t_{i+1}}$
\end_inset

 there,
 which is weird.
 The stochastic integral would be:
\begin_inset Formula 
\[
\int_{a}^{t}X_{s}dw_{s}=\lim_{Ind\left(S\right)\to0}I^{I}\left(s\right)
\]

\end_inset


\end_layout

\begin_layout Standard
What this requires is that 
\begin_inset Formula $w_{t}$
\end_inset

 be a Martingale,
 and that 
\begin_inset Formula $X_{t}$
\end_inset

 be not too awful.
 Read the book for the details,
 because like I mentioned,
 stochastic integrals are a whole course to themselves.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $I^{I}\left(s\right)$
\end_inset

,
 like we said,
 is defined with 
\begin_inset Formula $X_{t_{i}}$
\end_inset

 in the sum,
 and is the Ito integral.
 If we take 
\begin_inset Formula $X_{\frac{t_{i+1}+t_{i}}{2}}$
\end_inset

,
 we get the Stratonovich integral.
 They are not the same.
 We go with the Ito definition because the resulting process is a Martingale.
 But again,
 we don't actually explain any of this,
 and you need to read the book (or I may take some notes of that book in an appendix to these notes when I do so myself).
\end_layout

\begin_layout Subsubsection
Simple Example
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $w_{t}=t$
\end_inset

 be the deterministic time we are used to,
 and 
\begin_inset Formula $X_{t}$
\end_inset

 be discrete.
 In this case,
 it is clear why the integral converges,
 because it is simply the sum of the population so far.
\end_layout

\begin_layout Standard
Our goal is to define the branching process,
 where each individual has a reaction rate 
\begin_inset Formula $\lambda_{\nu}\left(X_{t},t\right)$
\end_inset

 to branch in to 
\begin_inset Formula $\nu$
\end_inset

 individuals.
 This is the multiplicity analog from before,
 where we combine both the number of progeny and the time rate of this type of effect.
\end_layout

\begin_layout Standard
Therefore,
 for each individual,
 the probability for a reaction that creates 
\begin_inset Formula $\nu$
\end_inset

 individuals in an interval 
\begin_inset Formula $\left[t,t+dt\right]$
\end_inset

 is 
\begin_inset Formula $\lambda_{\nu}\left(X_{t},t\right)dt+o\left(dt\right)$
\end_inset

.
\end_layout

\begin_layout Standard
We want to look at 
\begin_inset Formula $X_{t}-X_{0}$
\end_inset

.
 We take some partition 
\begin_inset Formula $S$
\end_inset

 as before of the time interval 
\begin_inset Formula $\left[0,t\right]$
\end_inset

.
 We can write:
\begin_inset Formula 
\[
X_{t}-X_{0}=\sum_{i=1}^{n}\left(X_{t_{i}}-X_{t_{i-1}}\right)=\sum_{i=1}^{n}\sum_{\nu=0}^{\infty}\left(\nu-1\right)Y_{\nu}\left[X_{t_{i}}\lambda_{\nu}\left(X_{t_{i}},t_{i}\right)dt+o\left(dt\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $Y_{\nu}$
\end_inset

 are independent Poisson random variables.
 This is because 
\begin_inset Formula $Y_{\nu}$
\end_inset

 are the counters for the exponential events with rates 
\begin_inset Formula $X_{t_{i}}\lambda_{\nu}\left(X_{t_{i}},t_{i}\right)$
\end_inset

,
 so they count the number of events in which the population change is 
\begin_inset Formula $\left(\nu-1\right)$
\end_inset

,
 and we sum over all possible options.
 The fact that we can do a sum over all possible options might demand something on the rates 
\begin_inset Formula $\lambda_{\nu}$
\end_inset

 for this to exist,
 but we're going to ignore that in this course for now.
 Use finite progeny numbers if you're feeling dirty.
\end_layout

\begin_layout Standard
We'll change the order of summation (again,
 we're not explaining what must happen for this to work):
\begin_inset Formula 
\[
X_{t}-X_{0}=\sum_{\nu=0}^{\infty}\left(\nu-1\right)\sum_{i=1}^{n}Y_{\nu}\left[X_{t_{i}}\lambda_{\nu}\left(X_{t_{i}},t_{i}\right)dt+o\left(dt\right)\right]=\sum_{\nu=0}^{\infty}\left(\nu-1\right)Y_{\nu}\left[\sum_{i=1}^{n}X_{t_{i}}\lambda_{\nu}\left(X_{t_{i}},t_{i}\right)dt+o\left(dt\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
And now we can write:
\begin_inset Formula 
\[
X_{t}-X_{0}=\sum_{\nu=0}^{\infty}\left(\nu-1\right)Y_{\nu}\left[\int_{0}^{t}X_{s}\lambda_{\nu}\left(X_{s},s\right)ds\right]
\]

\end_inset


\end_layout

\begin_layout Standard
So now can define things.
 
\begin_inset Formula $X_{t}$
\end_inset

 will be called a 
\emph on
continuous branching process
\emph default
 if there exist a series of 
\series bold
\bar under
positive
\series default
\bar default
 functions 
\begin_inset Formula $f_{k}\left(x,t\right)$
\end_inset

 such that:
\begin_inset Formula 
\[
X_{t}=X_{0}+\sum_{k}\left(k-1\right)Y_{k}\left[\int_{0}^{t}f_{k}\left(X_{s},s\right)ds\right]
\]

\end_inset


\end_layout

\begin_layout Standard
For independent Poisson random variables 
\begin_inset Formula $Y_{k}$
\end_inset

.
\end_layout

\begin_layout Section
21.4.2025
\end_layout

\begin_layout Standard
Last week was Passover,
 and next week is M&C2025,
 so we are sparsely having classes for now.
\end_layout

\begin_layout Standard
We will also introduce diffusion approximations in this course because 2/3 students are doing this.
\end_layout

\begin_layout Standard
We are changing the test at the end of the course to a home test.
\end_layout

\begin_layout Standard
And now back to continuous branching processes.
\end_layout

\begin_layout Subsection
The Master Equation
\end_layout

\begin_layout Standard
We remind that a branching process is given by:
\begin_inset Formula 
\[
X_{t}=X_{0}+\sum_{l=-1}^{N}lY_{l}\left[\int_{0}^{t}f_{l}\left(X_{s},s\right)ds\right]
\]

\end_inset


\end_layout

\begin_layout Standard
And we have this be a finite sum just because we don't want to deal with convergence because that's not the point here.
\end_layout

\begin_layout Standard
The branching laws 
\begin_inset Formula $f_{l}$
\end_inset

 are those who govern the distribution.
\end_layout

\begin_layout Standard
We define 
\begin_inset Formula $\lambda_{l}\left(X_{t},t\right)$
\end_inset

 as the rate per capita if 
\begin_inset Formula $f_{l}\left(X_{t},t\right)=\lambda_{l}\left(X_{t},t\right)X_{t}+S_{l}\left(t\right)$
\end_inset

.
 This should be taken as the rate per time for an individual in the population to undergo a reaction that yields an 
\begin_inset Formula $l$
\end_inset

-change in the population,
 in the short time frame 
\begin_inset Formula $\left[t,t+dt\right]$
\end_inset

.
 Therefore the probability is 
\begin_inset Formula $\mathbb{P}\left[\text{one individual doing l-interaction in \left[t,t+dt\right]}\right]=\lambda_{l}\left(X_{t},t\right)dt+o\left(dt\right)$
\end_inset

.
\end_layout

\begin_layout Standard
We now define,
 for any 
\begin_inset Formula $t_{2}>t_{1}$
\end_inset

:
\begin_inset Formula 
\[
H\left(t_{1},t_{2},n,k\right)=\mathbb{P}\left[X_{t_{2}}=n|X_{t_{1}}=k\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The backward Master Equation is therefore (The complete probability theorem):
\begin_inset Formula 
\[
\mathbb{P}\left[X_{t}=n\right]=\sum_{k}\mathbb{P}\left[X_{t-dt}=k\right]H\left(t-dt,t,n,k\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The forward Master Equation is similarly:
\begin_inset Formula 
\[
\mathbb{P}\left[X_{t}=n\right]=\sum_{k}\mathbb{P}\left[X_{dt}=k\right]H\left(dt,t,n,k\right)
\]

\end_inset


\end_layout

\begin_layout Standard
In a stationary system,
 where there is no direct dependence on the time,
 
\begin_inset Formula $H\left(t_{1},t_{2},n,k\right)=H\left(t_{2}-t_{1},n,k\right)$
\end_inset

.
 In that case,
 we can write the backward equation as:
\begin_inset Formula 
\[
\mathbb{P}\left[X_{t+dt}=n\right]=\sum_{k}\mathbb{P}\left[X_{t}=k\right]H\left(dt,n,k\right)
\]

\end_inset


\end_layout

\begin_layout Standard
and the forward as:
\begin_inset Formula 
\[
\mathbb{P}\left[X_{t+dt}=n\right]=\sum_{k}\mathbb{P}\left[X_{dt}=k\right]H\left(t,n,k\right)
\]

\end_inset


\end_layout

\begin_layout Standard
We will denote 
\begin_inset Formula $a_{n}\left(t\right)\equiv\mathbb{P}\left[X_{t}=n\right]$
\end_inset

,
 as usual.
 The backwards equation can be written as:
\begin_inset Formula 
\[
a_{n}\left(t+dt\right)=\sum_{k}a_{k}\left(t\right)H\left(dt,n,k\right)
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $H$
\end_inset

 can be written as:
 
\begin_inset Formula $H\left(dt,n,k\right)=f\left(n,k\right)dt+o\left(dt^{2}\right)$
\end_inset

,
 we get:
\begin_inset Formula 
\[
a_{n}\left(t+dt\right)=\sum_{k}a_{k}\left(t\right)f\left(n,k\right)dt+o\left(dt^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $f\left(n,n\right)=A+o\left(dt\right)$
\end_inset

,
 we can make this into an ODE.
\end_layout

\begin_layout Subsubsection
Constant per capita rates with no negative immigration
\end_layout

\begin_layout Standard
Lets start with the case where the per-capita rates are constant,
 so 
\begin_inset Formula $f_{l}\left(X_{t}\right)=\lambda_{l}X_{t}+S_{l}$
\end_inset

,
 where both 
\begin_inset Formula $\lambda_{l},S_{l}$
\end_inset

 are constant.
 We also demand that 
\begin_inset Formula $S_{-1}=S_{0}=0$
\end_inset

 (no non-positive immigration).
 The case of 
\begin_inset Formula $S_{0}$
\end_inset

 is just not very important,
 because it is a non-event.
\end_layout

\begin_layout Standard
In this case:
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $n\neq k$
\end_inset

,
 we can write:
\begin_inset Formula 
\[
H\left(dt,n,k\right)=\mathbb{P}\left[\text{One event takes us from \ensuremath{k} to \ensuremath{n} in \ensuremath{dt}}\right]=S_{n-k}dt+\lambda_{n-k}kdt+o\left(dt\right)
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $n=k$
\end_inset

,
 we can also have no event:
\begin_inset Formula 
\[
H\left(dt,n,n\right)=\cancelto{0}{\mathbb{P}\left[\text{Reaction of no change}\right]}+\mathbb{P}\left[\text{No event}\right]=1-\left(n\sum_{l=-1}^{N}\lambda_{l}-\sum_{l=1}^{N}S_{l}\right)dt
\]

\end_inset


\end_layout

\begin_layout Standard
We denote 
\begin_inset Formula $\lambda=\sum_{l}\lambda_{l},S=\sum_{l}S_{l}$
\end_inset


\end_layout

\begin_layout Standard
To make things simpler,
 we assumed both 
\begin_inset Formula $S_{0}=\lambda_{0}=0$
\end_inset

,
 which dropped the first term
\begin_inset Formula 
\[
a_{n}\left(t+dt\right)=\sum_{k\neq n}a_{k}\left(t\right)\left(k\lambda_{n-k}+S_{n-k}\right)dt+a_{n}\left(t\right)\left(1-n\lambda-S\right)dt+o\left(dt^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
And thus:
\begin_inset Formula 
\[
a_{n}\left(t+dt\right)-a_{n}\left(t\right)=-\left(n\lambda+S\right)a_{n}\left(t\right)dt+\sum_{k\neq n}a_{k}\left(t\right)\left(k\lambda_{n-k}+S_{n-k}\right)dt
\]

\end_inset


\end_layout

\begin_layout Standard
And divide by 
\begin_inset Formula $dt$
\end_inset

 and take the limit to get:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{da_{n}}{dt}\left(t\right)=-\left(n\lambda+S\right)a_{n}\left(t\right)+\sum_{k\neq n}a_{k}\left(t\right)\left(k\lambda_{n-k}+S_{n-k}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Which is a linear set of ODEs,
 which can be formulated with a matrix that has a sum over columns of 1.
\end_layout

\begin_layout Subsubsection
Onto the PGF!
\end_layout

\begin_layout Standard
We denote the PGF
\begin_inset Formula 
\[
G\left(x,t\right)=\sum_{n=0}^{\infty}a_{n}\left(t\right)x^{n}
\]

\end_inset


\end_layout

\begin_layout Standard
We also mark the PGF of the immigration by 
\begin_inset Formula $r\left(x\right)=\sum_{k=1}^{N}x^{k}S_{k}$
\end_inset

,
 and 
\begin_inset Formula $Q\left(x\right)=\sum_{k=1}^{N}\lambda_{k}x^{k}$
\end_inset

.
\end_layout

\begin_layout Standard
We multiply the ODE by 
\begin_inset Formula $x^{n}$
\end_inset

:
\begin_inset Formula 
\[
\frac{da_{n}}{dt}x^{n}=-\left(\lambda n+S\right)a_{n}\left(t\right)x^{n}+\sum_{k\neq n}x^{n}a_{k}\left(t\right)\left(k\lambda_{n-k}+S_{n-k}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
And sum over all possible values of 
\begin_inset Formula $n$
\end_inset

 to get (since the convergence to 
\begin_inset Formula $G$
\end_inset

 is uniform):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial G}{\partial t}\left(x,t\right) & =-\lambda x\sum_{n}nx^{n-1}a_{n}\left(t\right)-SG\left(x,t\right)+\sum_{n}\sum_{k\neq n}x^{k}a_{k}\left(t\right)\left(k\lambda_{n-k}x^{n-k}+S_{n-k}x^{n-k}\right)\\
 & =-\lambda x\frac{\partial G}{\partial x}\left(x,t\right)-SG\left(x,t\right)+\sum_{n}\sum_{k}x^{k}a_{k}\left(t\right)S_{n-k}x^{n-k}+\sum_{n}\sum_{k\neq n}x^{k}a_{k}\left(t\right)k\lambda_{n-k}x^{n-k}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The one before last is a multiplication of analytic functions (we added the 
\begin_inset Formula $k=n$
\end_inset

 term since 
\begin_inset Formula $S_{0}=0$
\end_inset

).
\begin_inset Formula 
\[
\frac{\partial G}{\partial t}\left(x,t\right)=-\lambda x\frac{\partial G}{\partial x}\left(x,t\right)-SG\left(x,t\right)+G\left(x,t\right)r\left(x\right)+x\sum_{n}\sum_{k}x^{k-1}a_{k}\left(t\right)k\lambda_{n-k}x^{n-k}
\]

\end_inset


\end_layout

\begin_layout Standard
We do a similar thing with the last term,
 except it is the partial derivative of 
\begin_inset Formula $G$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial G}{\partial t}\left(x,t\right)=-\lambda x\frac{\partial G}{\partial x}\left(x,t\right)-SG\left(x,t\right)+G\left(x,t\right)r\left(x\right)+x\frac{\partial G}{\partial x}\left(x,t\right)Q\left(x\right)
\]

\end_inset


\end_layout

\begin_layout Standard
To explain this more easily,
 we can keep the 
\begin_inset Formula $x^{n}$
\end_inset

 terms outside the convolution,
 and remember that 
\begin_inset Formula $ka_{k}$
\end_inset

 is the 
\begin_inset Formula $k$
\end_inset

-th term in 
\begin_inset Formula $x\frac{\partial G}{\partial x}$
\end_inset

,
 and thus from the convolution we can tell what the terms are.
\end_layout

\begin_layout Standard
It is often written as:
\begin_inset Formula 
\[
\frac{\partial G}{\partial t}=x\frac{\partial G}{\partial x}\left(x,t\right)\left(Q\left(x\right)-\lambda\right)+G\left(x,t\right)\left(r\left(x\right)-S\right)
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Let's do the equation for the first moment
\end_layout

\begin_layout Standard
As usual,
 
\begin_inset Formula $\mathbb{E}\left[\text{Population size}\right]=M_{1}=\frac{\partial G}{\partial x}\left(1,t\right)$
\end_inset

.
 We can write an ODE for it:
\begin_inset Formula 
\[
\frac{\partial^{2}G}{\partial t\partial x}\left(x,t\right)=\left(Q-\lambda\right)\left(\frac{\partial G}{\partial x}+x\frac{\partial^{2}G}{\partial x^{2}}\right)+x\frac{\partial G}{\partial x}\frac{dQ}{dx}+\frac{\partial G}{\partial x}\left(r-s\right)+G\frac{dr}{dx}
\]

\end_inset


\end_layout

\begin_layout Standard
We set 
\begin_inset Formula $x=1$
\end_inset

:
\begin_inset Formula 
\[
\frac{dM_{1}}{dt}=M_{1}\overline{\lambda_{r}}+\overline{S}
\]

\end_inset


\end_layout

\begin_layout Standard
Which is just the deterministic equation,
 if we had a constant reaction rate of the mean value and the immigration source was its mean constant.
\end_layout

\begin_layout Section
5.5.2025
\end_layout

\begin_layout Subsection
Form of the PGF for a branching process with rates and immigration
\end_layout

\begin_layout Standard
My thanks to Tomer for supplying the notes for this lesson,
 which I missed because my airline tried to leave me stranded in London indefinitely.
\end_layout

\begin_layout Standard
Define 
\begin_inset Formula $a_{n}\left(t\right)$
\end_inset

 as the probability of 
\begin_inset Formula $n$
\end_inset

 individuals at time 
\begin_inset Formula $t$
\end_inset

,
 assuming one individual at time 
\begin_inset Formula $t=0$
\end_inset

 and no immigration.
 Define 
\begin_inset Formula $A_{k}$
\end_inset

 as the event in which 
\begin_inset Formula $k$
\end_inset

 individuals were born in 
\begin_inset Formula $\left[0,dt\right]$
\end_inset

.
\end_layout

\begin_layout Standard
We do a backwards derivation for the events in 
\begin_inset Formula $\left[0,dt\right]$
\end_inset

 to get:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
a_{n}\left(t+dt\right)=\sum_{k}\mathbb{P}\left[X_{t}=n|A_{k}\right]\mathbb{P}\left[A_{k}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
From the per-capita definition of the rates,
 
\begin_inset Formula $\mathbb{P}\left[A_{k}\right]=\lambda_{k}dt$
\end_inset

,
 and 
\begin_inset Formula $\mathbb{P}\left[A_{0}\right]=\left(1-\lambda dt\right)$
\end_inset

:
\begin_inset Formula 
\[
a_{n}\left(t+dt\right)=\sum_{k}\sum_{\sum_{i=1}^{k}n_{i}=n}\prod_{i=1}^{k}a_{n_{i}}\left(t\right)\lambda_{k}dt+\left(1-\lambda dt\right)a_{n}\left(t\right)
\]

\end_inset


\begin_inset Formula 
\[
a_{n}\left(t+dt\right)-a_{n}\left(t\right)=-\lambda a_{n}\left(t\right)dt+\sum_{k}\sum_{\sum_{i=1}^{k}n_{i}=n}\prod_{i=1}^{k}a_{n_{i}}\left(t\right)\lambda_{k}dt
\]

\end_inset


\end_layout

\begin_layout Standard
So the ODE follows:
\begin_inset Formula 
\[
\frac{da_{n}}{dt}\left(t\right)=-\lambda a_{n}\left(t\right)+\sum_{k}\sum_{\sum_{i=1}^{k}n_{i}=n}\prod_{i=1}^{k}a_{n_{i}}\left(t\right)\lambda_{k}dt
\]

\end_inset


\end_layout

\begin_layout Standard
We can now multiply by 
\begin_inset Formula $x^{n}$
\end_inset

 and sum over all 
\begin_inset Formula $n$
\end_inset

 to get the PDE for the PGF 
\begin_inset Formula $g$
\end_inset

:
\begin_inset Formula 
\[
\frac{\partial g}{\partial t}\left(x,t\right)=-\lambda g\left(x,t\right)+\sum_{k}\lambda_{k}g^{k}\left(x,t\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Assume immigration starts at 
\begin_inset Formula $t=0$
\end_inset

 according to 
\begin_inset Formula $\mathbb{P}\left[k\text{ immigrants in \left[t,dt\right]}\right]=\gamma_{k}dt+o\left(dt\right)$
\end_inset

,
 and denote 
\begin_inset Formula $S=\sum\gamma_{k}$
\end_inset

.
\end_layout

\begin_layout Standard
We denote 
\begin_inset Formula $b_{n}\left(t\right)$
\end_inset

 as the probability of 
\begin_inset Formula $n$
\end_inset

 individuals at time 
\begin_inset Formula $t$
\end_inset

 for this new population.
 We do a similar derivation with the law of total probability:
\begin_inset Formula 
\[
b_{n}\left(t+dt\right)=b_{n}\left(t\right)\left[1-Sdt\right]+\sum_{k}\gamma_{k}dt\left[\sum_{i+m=n}b_{i}\left(t\right)\prod_{\sum_{j=1}^{k}m_{j}=m}a_{m_{j}}\left(t\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
We used a tricky thing here.
 If 
\begin_inset Formula $b$
\end_inset

 isn't just immigration but the total,
 the inner sum that decouples 
\begin_inset Formula $a,b$
\end_inset

 does not work unless we can tell certain individuals have no immigration relating to them.
 We are,
 in essence,
 saying that the first 
\begin_inset Formula $k$
\end_inset

 individuals grow with no immigration and say 
\begin_inset Quotes eld
\end_inset

Oh,
 there's some additional thing going on from other contributions
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
The ODE for 
\begin_inset Formula $b$
\end_inset

 is given by:
\begin_inset Formula 
\[
\frac{db_{n}}{dt}\left(t\right)=-Sb_{n}\left(t\right)+\sum_{k}\gamma_{k}\left[\sum_{i+m=n}b_{i}\left(t\right)\prod_{\sum_{j=1}^{k}m_{j}=m}a_{m_{j}}\left(t\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
If we denote 
\begin_inset Formula $G$
\end_inset

 as the PGF for this case,
 we get:
\begin_inset Formula 
\[
\frac{\partial G}{\partial t}\left(x,t\right)=-SG\left(x,t\right)+\sum_{n}x^{n}\sum_{k}\gamma_{k}\left[\sum_{i+m=n}b_{i}\left(t\right)\prod_{\sum_{j=1}^{k}m_{j}=m}a_{m_{j}}\left(t\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
We notice that 
\begin_inset Formula $\left[g^{k}\left(x,t\right)\right]_{m}=\prod_{\sum_{j=1}^{k}m_{j}=m}a_{m_{j}}\left(t\right)$
\end_inset

,
 and therefore:
\begin_inset Formula 
\[
\sum_{i+m=n}b_{i}\left(t\right)\prod_{\sum_{j=1}^{k}m_{j}=m}a_{m_{j}}\left(t\right)=\left[g^{k}G\right]_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
Thus:
\begin_inset Formula 
\begin{align*}
\frac{\partial G}{\partial t}\left(x,t\right) & =-SG\left(x,t\right)+\sum_{k}\gamma_{k}\sum_{n}x^{n}\left[g^{k}G\right]_{n}=-SG\left(x,t\right)+\sum_{k}\gamma_{k}G\left(x,t\right)g^{k}\left(x,t\right)=\\
 & G\left(x,t\right)\left[-S+\sum_{k}\gamma_{k}g^{k}\left(x,t\right)\right]
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And therefore we can solve:
\begin_inset Formula 
\[
G\left(x,t\right)=C\left(x\right)\exp\left(\int_{0}^{t}\left[-S+\sum_{k}\gamma_{k}g^{k}\left(x,s\right)\right]ds\right)
\]

\end_inset


\end_layout

\begin_layout Subsection
Moment Equations
\end_layout

\begin_layout Standard
We denote lower case 
\begin_inset Formula $m,f$
\end_inset

 as the uncentered moments and factorial moments of 
\begin_inset Formula $g$
\end_inset

,
 and upper case 
\begin_inset Formula $M,F$
\end_inset

 for 
\begin_inset Formula $G$
\end_inset

.
\begin_inset Formula 
\[
\frac{\partial^{2}g}{\partial t\partial x}\left(x,t\right)=-\lambda\frac{\partial g}{\partial x}\left(x,t\right)+\sum_{k}\lambda_{k}kg^{k-1}\left(x,t\right)\frac{\partial g}{\partial x}\left(x,t\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Set 
\begin_inset Formula $x=1$
\end_inset

 and we get:
\begin_inset Formula 
\[
\frac{dm_{1}}{dt}=-\lambda m_{1}+\sum_{k}\lambda_{k}km_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
Denote 
\begin_inset Formula $\alpha=\sum\lambda_{k}k-\lambda$
\end_inset

 to get:
\begin_inset Formula 
\[
\frac{dm_{1}}{dt}=\alpha m_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
and thus
\begin_inset Formula 
\[
m_{1}=e^{\alpha t}
\]

\end_inset


\end_layout

\begin_layout Standard
We know that 
\begin_inset Formula $f_{2}=m_{2}-m_{1}$
\end_inset

,
 and we do the second derivative:
\begin_inset Formula 
\[
\frac{\partial^{3}g}{\partial t\partial x^{2}}=-\lambda\frac{\partial^{2}g}{\partial x^{2}}+\sum_{k}\lambda_{k}k\left[\left(k-1\right)g^{k-2}\left(\frac{\partial g}{\partial x}\right)^{2}+g^{k-1}\frac{\partial^{2}g}{\partial x^{2}}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
And setting 
\begin_inset Formula $x=1$
\end_inset

 again we get:
\begin_inset Formula 
\[
\frac{df_{2}}{dt}=-\lambda f_{2}+\sum_{k}\lambda_{k}k\left[\left(k-1\right)e^{2\alpha t}+f_{2}\right]=\alpha f_{2}+e^{2\alpha t}\sum\lambda_{k}k\left(k-1\right)
\]

\end_inset


\end_layout

\begin_layout Standard
We denote 
\begin_inset Formula $\sum\lambda_{k}k\left(k-1\right)\equiv D_{2}$
\end_inset

 to get:
\begin_inset Formula 
\[
\frac{df_{2}}{dt}=\alpha f_{2}+D_{2}e^{2\alpha t}
\]

\end_inset


\end_layout

\begin_layout Standard
We solve the homogeneous and particular solutions:
\begin_inset Formula 
\[
f_{2h}=Ce^{\alpha t},f_{2p}=Ae^{2\alpha t}
\]

\end_inset


\end_layout

\begin_layout Standard
and solve for 
\begin_inset Formula $A,C$
\end_inset

 to get:
\begin_inset Formula 
\[
f_{2}\left(t\right)=\frac{D_{2}}{\alpha}\left(e^{2\alpha t}-e^{\alpha t}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Using the form of 
\begin_inset Formula $G$
\end_inset

 and denoting 
\begin_inset Formula $Q\left(x,t\right)=\int_{0}^{t}\left[-S+\sum_{k}\gamma_{k}g^{k}\left(x,s\right)\right]ds$
\end_inset

,
 we can get (
\series bold
if we ignore 
\begin_inset Formula $c\left(x\right)$
\end_inset

!
\series default
):
\begin_inset Formula 
\[
\frac{\partial G}{\partial x}=\frac{\partial Q}{\partial x}e^{Q\left(x,t\right)}=\left(\int_{0}^{t}ds\sum_{k}\gamma_{k}kg^{k-1}\frac{\partial g}{\partial x}\right)e^{Q}
\]

\end_inset


\end_layout

\begin_layout Standard
Set 
\begin_inset Formula $x=1$
\end_inset

:
\begin_inset Formula 
\[
\frac{dM_{1}}{dt}=\left(\int_{0}^{t}\sum_{k}\gamma_{k}ke^{\alpha s}ds\right)1=\frac{\left(e^{\alpha t}-1\right)}{\alpha}\sum_{k}\gamma_{k}k
\]

\end_inset


\end_layout

\end_body
\end_document
